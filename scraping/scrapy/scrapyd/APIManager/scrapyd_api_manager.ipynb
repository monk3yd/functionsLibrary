{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Scrapyd's API Manager\n",
    "    # https://scrapyd.readthedocs.io/en/latest/api.html\n",
    "\n",
    "    # logging: debug info warning error critical\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logging.info(\"Initialize Scrapyd's API Manager\")\n",
    "\n",
    "    SLEEP_TIME = 180\n",
    "    logging.info(f\"SLEEP_TIME_IN_SECONDS: {SLEEP_TIME}\")\n",
    "\n",
    "    LISTJOBS_PATH = \"/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/listjobs.json\"\n",
    "    logging.info(f\"LISTJOBS_FILE_PATH: {LISTJOBS_PATH}\")\n",
    "\n",
    "    DAEMONSTATUS_PATH = \"/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/daemonstatus.json\"\n",
    "    logging.info(f\"DAEMONSTATUS_FILE_PATH: {DAEMONSTATUS_PATH}\")\n",
    "\n",
    "\n",
    "    logging.info(\"Listening to scrapyd server...\")\n",
    "    while True:\n",
    "        logging.info(\"Checking status...\")\n",
    "\n",
    "        # --- DaemonStatus ---\n",
    "        logging.info(\"Save daemonstatus...\")\n",
    "        save_daemonstatus(DAEMONSTATUS_PATH)\n",
    "\n",
    "        # --- ListJobs ---\n",
    "        logging.info(\"Save listjobs...\")\n",
    "        save_listjobs(LISTJOBS_PATH)\n",
    "       \n",
    "        # W8 for next check\n",
    "        logging.info(f\"Time for next status report: {SLEEP_TIME} seconds...\\n\")\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # --- Schedule ---\n",
    "    # spider_data = {}\n",
    "    # add_to_schedule(spider_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_listjobs(file_path):\n",
    "    '''\n",
    "    Get the list of pending, running and finished jobs of some project.\n",
    "\n",
    "    Supported Request Methods: GET\n",
    "    Parameters:\n",
    "        project (string, option) - restrict results to project name\n",
    "    '''\n",
    "\n",
    "    url = \"http://127.0.0.1:6800/listjobs.json\"\n",
    "    response = requests.get(url=url)\n",
    "\n",
    "    # Update listjobs JSON file\n",
    "    logging.info(\"Saving listjobs in JSON file...\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(json.dumps(response.json(), indent=4))\n",
    "\n",
    "    # Read new datra in listjobs\n",
    "    logging.info(\"Reading listjobs JSON file...\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        json_data = json.loads(file.read())\n",
    "\n",
    "    pending_jobs = json_data[\"pending\"]\n",
    "    running_jobs = json_data[\"running\"]\n",
    "    finished_jobs = json_data[\"finished\"]  # List of dicts\n",
    "\n",
    "    if not pending_jobs and not running_jobs and not finished_jobs:\n",
    "        logger.warning(\"Couldn't find any jobs in listjobs...\")  # listjobs is empty\n",
    "        sys.exit()\n",
    "\n",
    "    # Jobs finished\n",
    "    if not pending_jobs and not running_jobs and finished_jobs:\n",
    "        finished_df = pd.DataFrame(finished_jobs)\n",
    "        \n",
    "        # Save to db\n",
    "        connect_to_sql(merged_df)\n",
    "    \n",
    "    # Jobs almost finished\n",
    "    if not pending_jobs and running_jobs and finished_jobs:\n",
    "        running_df = pd.DataFrame(running_jobs)\n",
    "        finished_df = pd.DataFrame(finished_jobs)\n",
    "        \n",
    "        # Merge\n",
    "        # merged_df = finished_df.merge(running_df, how=\"outer\")\n",
    "\n",
    "        # Save to db\n",
    "        # connect_to_sql(merged_df)\n",
    "    \n",
    "    # Jobs are stale (not scraping)\n",
    "    if pending_jobs and not running_jobs and finished_jobs:\n",
    "        pending_df = pd.DataFrame(pending_jobs)\n",
    "        finished_df = pd.DataFrame(finished_jobs)\n",
    "        \n",
    "        # Merge\n",
    "        # merged_df = finished_df.merge(pending_df, how=\"outer\")\n",
    "\n",
    "        # Save to db\n",
    "        # connect_to_sql(merged_df)\n",
    "    \n",
    "    # Fully scraping jobs\n",
    "    if pending_jobs and running_jobs and finished_jobs:\n",
    "        pending_df = pd.DataFrame(pending_jobs)\n",
    "        running_df = pd.DataFrame(running_jobs)\n",
    "        finished_df = pd.DataFrame(finished_jobs)\n",
    "        \n",
    "        # Merge\n",
    "        merged_df = finished_df.merge(running_df, how=\"outer\").merge(pending_df, how=\"outer\")\n",
    "\n",
    "        # Save to db\n",
    "        connect_to_sql(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_sql(dataframe):\n",
    "    database = \"../dbs/listjobs.db\"\n",
    "    logging.info(f\"Create listjobs database at {database}...\")\n",
    "    conn = sqlite3.connect(database)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # If database doesn't exist\n",
    "    if not os.path.exists(database):\n",
    "        # Create db from dataframe\n",
    "        dataframe.to_sql(name=\"listjobs\", con=conn)\n",
    "        \n",
    "    else:\n",
    "        # Read db into dataframe\n",
    "        database_df = pd.read_sql(\"SELECT * FROM listjobs\", conn)\n",
    "\n",
    "        # Get delta of updated_df - database_df\n",
    "        # delta_df = dataframe - database_df\n",
    "\n",
    "        #\n",
    "        # TODO: Update existing entries\n",
    "        cur.execute(\"UPDATE listjobs \")\n",
    "        # TODO: Add new entries\n",
    "        ...\n",
    "\n",
    "    conn.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_daemonstatus(file_path):\n",
    "    '''\n",
    "    Load status of a service.\n",
    "\n",
    "    Supported Request Methods: GET\n",
    "\n",
    "    '''\n",
    "    url = \"http://localhost:6800/daemonstatus.json\"\n",
    "    response = requests.get(url=url)\n",
    "    \n",
    "    # Save into json\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(json.dumps(response.json(), indent=4))\n",
    "    return 0\n",
    "\n",
    "    # TODO: save into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialize Scrapyd's API Manager\n",
      "INFO:root:SLEEP_TIME_IN_SECONDS: 180\n",
      "INFO:root:LISTJOBS_FILE_PATH: /home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/listjobs.json\n",
      "INFO:root:DAEMONSTATUS_FILE_PATH: /home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/daemonstatus.json\n",
      "INFO:root:Listening to scrapyd server...\n",
      "INFO:root:Checking status...\n",
      "INFO:root:Save daemonstatus...\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:6800\n",
      "DEBUG:urllib3.connectionpool:http://localhost:6800 \"GET /daemonstatus.json HTTP/1.1\" 200 89\n",
      "INFO:root:Save listjobs...\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 127.0.0.1:6800\n",
      "DEBUG:urllib3.connectionpool:http://127.0.0.1:6800 \"GET /listjobs.json HTTP/1.1\" 200 2655725\n",
      "INFO:root:Saving listjobs in JSON file...\n",
      "INFO:root:Reading listjobs JSON file...\n",
      "INFO:root:Create listjobs database at ../dbs/listjobs.db...\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM listjobs': no such table: listjobs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:2020\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2020\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2021\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: listjobs",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb Cell 6\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# --- ListJobs ---\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSave listjobs...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m save_listjobs(LISTJOBS_PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# W8 for next check\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime for next status report: \u001b[39m\u001b[39m{\u001b[39;00mSLEEP_TIME\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb Cell 6\u001b[0m in \u001b[0;36msave_listjobs\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m merged_df \u001b[39m=\u001b[39m finished_df\u001b[39m.\u001b[39mmerge(running_df, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmerge(pending_df, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Save to db\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m connect_to_sql(merged_df)\n",
      "\u001b[1;32m/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb Cell 6\u001b[0m in \u001b[0;36mconnect_to_sql\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     dataframe\u001b[39m.\u001b[39mto_sql(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlistjobs\u001b[39m\u001b[39m\"\u001b[39m, con\u001b[39m=\u001b[39mconn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Read db into dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     database_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT * FROM listjobs\u001b[39;49m\u001b[39m\"\u001b[39;49m, conn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Get delta of updated_df - database_df\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# delta_df = dataframe - database_df\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# TODO: Update existing entries\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/scrapyd_api_manager.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     cur\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mUPDATE listjobs \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:566\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    563\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    567\u001b[0m         sql,\n\u001b[1;32m    568\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    569\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    570\u001b[0m         coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    571\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    572\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    575\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     _is_table_name \u001b[39m=\u001b[39m pandas_sql\u001b[39m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:2080\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[1;32m   2069\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2077\u001b[0m ):\n\u001b[1;32m   2079\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2080\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   2081\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[1;32m   2083\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py:2032\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM listjobs': no such table: listjobs"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daemonstatus\n",
    "Load status of a service.\n",
    "Supported Request Methods: GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>node_name</th>\n",
       "      <td>RFTL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>status</th>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pending</th>\n",
       "      <td>11013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>running</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finished</th>\n",
       "      <td>8660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "node_name   RFTL\n",
       "status        ok\n",
       "pending    11013\n",
       "running       32\n",
       "finished    8660"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daemonstatus dataframe\n",
    "daemonstatus_path = \"/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/daemonstatus.json\"\n",
    "with open(daemonstatus_path, \"r\") as file:\n",
    "    json_data = json.loads(file.read())\n",
    "\n",
    "daemonstatus_df = pd.DataFrame.from_dict(json_data, orient=\"index\")\n",
    "daemonstatus_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listjobs\n",
    "Get the list of pending, running and finished jobs of some project.\n",
    "\n",
    "Supported Request Methods: GET\n",
    "Parameters:\n",
    "    project (string, option) - restrict results to project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listjobs dataframe\n",
    "listjobs_path = \"/home/monk3yd/GDrive/theLab/work/pjud_scraper/scrapyd_APIManager/services/listjobs.json\"\n",
    "with open(listjobs_path, \"r\") as file:\n",
    "    json_data = json.loads(file.read())\n",
    "\n",
    "server_name = json_data[\"node_name\"]  # str\n",
    "server_status = json_data[\"status\"]  # str\n",
    "\n",
    "# Dataframes\n",
    "pending_jobs = json_data[\"pending\"]  # List of dicts\n",
    "if pending_jobs:\n",
    "    pending_df = pd.DataFrame(pending_jobs)\n",
    "\n",
    "running_jobs = json_data[\"running\"]  # List of dicts\n",
    "if running_jobs:\n",
    "    running_df = pd.DataFrame(running_jobs)\n",
    "\n",
    "finished_jobs = json_data[\"finished\"]  # List of dicts\n",
    "if finished_jobs:\n",
    "    finished_df = pd.DataFrame(finished_jobs)\n",
    "\n",
    "# Number of rows (finished causas)\n",
    "len(finished_df.index)\n",
    "\n",
    "finished_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "super_df = finished_df.merge(running_df, how=\"outer\").merge(pending_df, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite connection\n",
    "database = \"dbs/listjobs.db\"\n",
    "conn = sqlite3.connect(database)\n",
    "super_df.to_sql(name=\"listjobs\", con=conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_daemonstatus(file_path):\n",
    "    '''\n",
    "    Load status of a service.\n",
    "\n",
    "    Supported Request Methods: GET\n",
    "\n",
    "    '''\n",
    "    url = \"http://localhost:6800/daemonstatus.json\"\n",
    "    response = requests.get(url=url)\n",
    "    \n",
    "    # Save into json\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(json.dumps(response.json(), indent=4))\n",
    "    return 0\n",
    "\n",
    "    # TODO: save into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_listjobs(file_path):\n",
    "    '''\n",
    "    Get the list of pending, running and finished jobs of some project.\n",
    "\n",
    "    Supported Request Methods: GET\n",
    "    Parameters:\n",
    "        project (string, option) - restrict results to project name\n",
    "    '''\n",
    "\n",
    "    url = \"http://127.0.0.1:6800/listjobs.json\"\n",
    "    response = requests.get(url=url)\n",
    "\n",
    "    # Save into json\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(json.dumps(response.json(), indent=4))\n",
    "\n",
    "    # TODO: Save into db\n",
    "\n",
    "    # Listjobs dataframe\n",
    "    logging.info(\"Save JSON...\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        json_data = json.loads(file.read())\n",
    "\n",
    "    # SQLite connection\n",
    "    # connect_to_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_sql():\n",
    "    # List of dicts\n",
    "    pending_listjobs = json_data[\"pending\"]\n",
    "    if pending_listjobs:\n",
    "        pending_df = pd.DataFrame(pending_listjobs)\n",
    "        pending_database = \"dbs/pending.db\"\n",
    "        conn = sqlite3.connect(pending_database)\n",
    "        pending_df.to_sql(name=\"pending_listjobs\", con=conn)\n",
    "\n",
    "    running_listjobs = json_data[\"running\"]\n",
    "    if running_listjobs:\n",
    "        running_df = pd.DataFrame(running_listjobs)\n",
    "        running_database = \"dbs/running.db\"\n",
    "        conn = sqlite3.connect(running_database)\n",
    "        running_df.to_sql(name=\"running_listjobs\", con=conn)\n",
    "\n",
    "    finished_listjobs = json_data[\"finished\"]\n",
    "    if finished_listjobs:\n",
    "        finished_df = pd.DataFrame(finished_listjobs)\n",
    "        finished_database = \"dbs/finished.db\"\n",
    "        conn = sqlite3.connect(finished_database)\n",
    "        finished_df.to_sql(name=\"finished_listjobs\", con=conn)\n",
    "\n",
    "    # TODO: Merge dataframes\n",
    "    try:\n",
    "        super_df = finished_df.merge(running_df, how=\"outer\").merge(pending_df, how=\"outer\")\n",
    "        if not super_df.empty:\n",
    "            logging.info(\"Create listjobs database...\")\n",
    "            database = \"dbs/listjobs.db\"\n",
    "            conn = sqlite3.connect(database)\n",
    "            # If database doesn't exist\n",
    "            if not os.path.exists(database):\n",
    "                # Create db from dataframe\n",
    "                super_df.to_sql(name=\"listjobs\", con=conn)\n",
    "            else:\n",
    "                # TODO: Read db into dataframe\n",
    "                database_df = pd.read_sql(\"SELECT * FROM listjobs\", conn)\n",
    "\n",
    "                # TODO: Get delta of db dataframe with super/updated dataframe\n",
    "                delta_df = super_df - database_df\n",
    "\n",
    "                # TODO: Update existing entries\n",
    "                # TODO: Add new entries\n",
    "                ...\n",
    "\n",
    "            conn.close()\n",
    "            return 0\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_schedule(spider_data):\n",
    "    '''\n",
    "    # Schedule:\n",
    "    Schedule a spider run (also known as a job), returning the job id.\n",
    "    \n",
    "    Supported Request Methods: POST\n",
    "    Parameters:\n",
    "        project (string, required) - the project name\n",
    "        spider (string, required) - the spider name\n",
    "        setting (string, optional) - a Scrapy setting to use when running the spider\n",
    "        jobid (string, optional) - a job id used to identify the job, overrides the default generated UUID\n",
    "        priority (float, optional) - priority for this project’s spider queue — 0 by default\n",
    "        _version (string, optional) - the version of the project to use\n",
    "        any other parameter is passed as spider argument\n",
    "    '''\n",
    "\n",
    "    url = \"http://127.0.0.1:6800/schedule.json\"\n",
    "    response = requests.post(url=url, params=spider_data)\n",
    "    return response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72e27a53c9feb3cf5a516ba60a04465e43171845b903b016020c3a9c1393c532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
